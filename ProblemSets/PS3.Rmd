---
title: "PS3"
author: "Wanlin Ji"
date: "5/15/2017"
output: html_document
---

```{r}
rm(list = ls())
setwd('/Users/jiwanlin/Desktop/MACS30200proj/ProblemSets')
dat <- read.csv('biden.csv')
datAB <- na.omit(dat) 
```

# Regression diagnostics (5 pts)
## 1.Test the model to identify any unusual and/or influential observations. 
```{r}
modelA <- lm(biden ~ age + female + educ, data = datAB)
summary(modelA)
```

# Assessing outliers

```{r}
library(car)
outlierTest(modelA)
```

This functin only judges whether there is outliers by the significance level of residual. If not significant, it tells me that there is no outliers in the data; if significant, then we must look into the reason for this outlier and examine whether there are other significant outliers existing. The result showed that Point 34 is diagnosed as outlier(p = 0.003), if we can be sure according to the original material that this point is due to an error, we can delete this point and use other cases for further analysis.

# Cook's D Plot 
# identify D values > 4/(n-k-1)

```{r}
cutoff <- 4/(nrow(datAB) - length(modelA$coefficients) - 2)
plot(modelA, which = 4, cook.levels = cutoff)
abline(h = cutoff, lty = 2, col = "red")
```

Cook's D can detect whether there is influencial point, which would cause large influence to the model when removing it.  From the Cook's D test, an influencial point is when D is larger than 4/(n-k-1). n stands for size of the sample and k is the number of predictive variables. From the graph, we can find that Point 597, 885, and 1447 belongs to influencial points. Removing these three points would cause huge difference to the model predictions. 

Firstly, we can removing them individually and observe the difference, then we look into the original data collection materials to find the reason for the influencial points. We are free to delete these points if they are due to error. But in other circumstances, we may need to remove the data point to see if there is any influence, produce the figure for comparing coefficient changes with/without outliers and report in the footnote. 


# 2.Test for non-normally distributed errors. 
# Assessing normality

```{r}
qqPlot(modelA, labels = FALSE, simulate = TRUE, main = "Q-Q Plot")
```

We can find the central part of points are in the 95 percent confidence interval that fits the normal distribution assumption and we can also find that the points in the edge are not satisfying the assumption and shows the shape quite like the uniform distribution. We need to inspect the reason for producing these y values in out data. Again, if it belongs to error we should delete them, or keep them and leave an explanation.

# 3.Test for heteroscedasticity in the model. 
# Assessing homoscedasticity
```{r}
ncvTest(modelA)
```

```{r}
spreadLevelPlot(modelA)
```
Suggested power transformation:  0.9305313

From the result we can find that the the significance level is 0.046, which is marginal significance and satisfies the assumption. From the graph, we can find that the graph is roughly aroung the horizontal line, but some points are far away from the horizontal curves. 

# 4.Test for multicollinearity.
```{r}
vif(modelA)
```

```{r}
sqrt(vif(modelA)) > 2
```

From the result we find there is no multicollinearity with these three variables.



# Interaction terms (5 pts)
```{r}
modelB <- lm(biden ~ age + educ + age:educ, data = datAB)
summary(modelB)
```

# 1.effect of age on Joe Biden thermometer rating, conditional on education. 

From the result, age has the significant positive influence to the Joe Biden thermometer, the coefficient is 0.67 (p < 0.001), and its interaction term with education is negative. When the ages is larger, the education has less positive influence, the interaction term coefficient is -0.05 (p<0.001).

# 2.effect of education on Joe Biden thermometer rating, conditional on age. 

Education has a significant level of influence to Joe Biden thermometer, the influence is positive with an coefficient is 1.65 (p<0.001), the education decrease the influence that age may have on the thermometer, the interaction term is -0.05 (p<0.001).

# Missing data (5 pts)
# multiple imputation
```{r}
library(mice)
imp <- mice(dat, seed = 1234)
```

```{r}
modelC <- with(imp, lm(biden ~ age + female + educ))
pooled <- pool(modelC)
summary(pooled)
```

```{r}
imp
```

```{r}
datP <- complete(imp, action=3)
head(dat)
```

The result after imputation shows that the intercept is 65.32 (p<0.001), age has the influencial coefficient 0.05 (p<0.001) to the thermometer with significance. The gender also has a significant influence with coefficient 5.44 (p<0.001). The education has a a significant influence with coefficient -0.65 (p<0.001).

For the non-imputed model, the intercept is 68.62 (p<0.001). Age does not have a significant influence on the model with coefficient 0.04 (p=0.2). Gender has a significant level of influence with coefficient -0.88(p<0.001). Generally, the direction of coefficient did not changed after imputation, but the coefficient did change slightly due to that deleting the missing value decrease the observations. And it also effect the significant level. The age is significant after imputation but for the non-imputed model the age is not significant. The standard errors also changed slightly.

